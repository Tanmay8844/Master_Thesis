import gym
import numpy as np
import random
import tensorflow as tf
from tensorflow.keras import layers, models
from collections import deque


import base64
import imageio
import IPython
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import PIL.Image
import pyvirtualdisplay
import reverb

import tensorflow as tf

from tf_agents.agents.dqn import dqn_agent
from tf_agents.drivers import py_driver
from tf_agents.environments import suite_gym
from tf_agents.environments import tf_py_environment
from tf_agents.eval import metric_utils
from tf_agents.metrics import tf_metrics
from tf_agents.networks import sequential
from tf_agents.policies import py_tf_eager_policy
from tf_agents.policies import random_tf_policy
from tf_agents.replay_buffers import reverb_replay_buffer
from tf_agents.replay_buffers import reverb_utils
from tf_agents.trajectories import trajectory
from tf_agents.specs import tensor_spec
from tf_agents.utils import common

data_save_folder = "C://Users//Admin//OneDrive//Desktop//Thesis work//train_data_1//"
for ex in range(9):
    
    #if ex==0: file_in = 'iAllactivityhorTan.csv'    # 
    #if ex==1: file_in = 'iAllactivityTan2.csv'    # 
    
    
    #if ex==0: file_in = 'iAllactivityTan.csv'    #  261 GDH
    
    #if ex==0: file_in = 'iAllactivityboyang.csv'    #  261 GDH        S3
    #if ex==1: file_in = 'iAllactivityboyang2.csv'
    #if ex==2: file_in = 'iAllactivityTan.csv'    #  261 GDH        S3
    
    #if ex==0: file_name = 'allactivity_person1_1.csv'    #  261 GDH        S3
    
    #if ex==1: file_name = 'allactivity_person1_2.csv'    # 
    
    #if ex==2: file_name = 'allactivity_person2_1_hor.csv' 
    
    #if ex==3: file_name = 'allactivity_person2_1.csv' 
    #if ex==4: file_name = 'allactivity_person3_1.csv' 
      
    file_in = 'C://Users//Admin//OneDrive//Desktop//Thesis work//Data_Acquisition//after insert&delete//data/' +file_name


# Define the CSI Image Classification Environment
class CSIITIVEnv(gym.Env):
    def __init__(self):
        super(CSIITIVEnv, self).__init__()
        action_label_mapping = {
          "sit" : 0,
          "stand" : 1,
          "walk" : 2,
          "Empty" : 3
        }

        self.action_space = gym.spaces.Discrete(4)  # Four classes: Sit, Walk, Stand, Empty
        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(224, 224, 3), dtype=np.float224)  # CSI image

        self.current_image = None
        self.correct_class = None

    def reset(self):
        # Generate a CSI image (224x224 pixels with values in [0, 255])
        self.current_image = np.random.rand(224, 224, 3).astype(np.float32)
        # assign the classes
        self.correct_class = np.random.randint(0, 4)
        return self.current_image

    def step(self, action):
        # Calculate reward based on classification correctness
        is_correct = (action == self.correct_class)
        reward = 0.1 if is_correct else -0.1

        done = True  # End the episode after a single step

        return self.current_image, reward, done, {}

    def render(self, mode='self'):
        # Render the CSI image (optional)
        pass

# Create a TF environment from the Gym environment
tf_env = tf_py_environment.TFPyEnvironment(suite_gym.load("CSIITIVEnv-v0"))

# DQN parameters
num_actions = 4  # Four classes: Sit, Walk, Stand, Empty
replay_memory_capacity = 1000
batch_size = 32
epsilon_initial = 1.0
epsilon_final = 0.1
epsilon_decay_steps = 1000
discount_factor = 0.9
learning_rate = 0.001
target_update_frequency = 10

# Create a DQN model
def create_dqn_model(input_shape, num_actions):
    model = models.Sequential([
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Flatten(),
        layers.Dense(64, activation='relu'),
        layers.Dense(num_actions, activation='linear')  # Change softmax to linear
    ])
    return model

# Initialize the DQN model and target model
input_shape = (224, 224, 3)
dqn_model = create_dqn_model(input_shape, num_actions)
target_model = create_dqn_model(input_shape, num_actions)
target_model.set_weights(dqn_model.get_weights())

# Initialize replay memory
replay_memory = deque(maxlen=replay_memory_capacity)

# Initialize epsilon for epsilon-greedy exploration
epsilon = epsilon_initial

# Q-learning training
num_episodes = 1000

for episode in range(num_episodes):
    time_step = tf_env.reset()
    state = time_step.observation.numpy()
    done = False
    total_reward = 0

    while not done:
        # Choose an action using epsilon-greedy strategy
        if np.random.rand() < epsilon:
            action = np.random.randint(0, num_actions)  # Explore
        else:
            q_values = dqn_model.predict(np.expand_dims(state, axis=0))
            action = np.argmax(q_values)  # Exploit

        # Take the chosen action
        next_time_step = tf_env.step(action)
        next_state = next_time_step.observation.numpy()
        reward = next_time_step.reward.numpy()
        done = next_time_step.is_last()

        # Store the experience in replay memory
        replay_memory.append((state, action, reward, next_state, done))

        # Sample a random batch from replay memory for training
        if len(replay_memory) >= batch_size:
            batch = random.sample(replay_memory, batch_size)
            states, actions, rewards, next_states, dones = zip(*batch)

            # Calculate target Q-values using the target network
            target_q_values = target_model.predict(np.array(next_states))
            max_target_q_values = np.max(target_q_values, axis=1)
            targets = np.where(dones, rewards, rewards + discount_factor * max_target_q_values)

            # Update the DQN model
            dqn_model.fit(np.array(states), np.array(actions), verbose=0, sample_weight=targets)

            # Update epsilon for epsilon-greedy exploration
            if epsilon > epsilon_final:
                epsilon -= (epsilon_initial - epsilon_final) / epsilon_decay_steps

        state = next_state
        total_reward += reward

    print(f"Episode {episode + 1} completed. Total Reward: {total_reward}")

    # Update the target network periodically
    if (episode + 1) % target_update_frequency == 0:
        target_model.set_weights(dqn_model.get_weights())
